{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cebra\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import itertools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#check if cuda is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = 'models/cebra_model_complete.pt'\n",
    "neural_data_directory= []\n",
    "behavior_data_directory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '/mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/'\n",
    "neural_data_paths = [ data_directory + 'brain/' + \\\n",
    "                     file for file in os.listdir(data_directory + 'brain/')]\n",
    "\n",
    "behavior_data_paths = [  data_directory + 'camera1/' + \\\n",
    "                     file for file in os.listdir(data_directory + 'brain/')]\n",
    "\n",
    "dino_paths = [ data_directory + 'dino/' + \\\n",
    "                        file for file in os.listdir(data_directory + 'brain/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_brain(brain_seq):\n",
    "  brain_seq = np.array(brain_seq)\n",
    "  flat_seq = np.array([(brain_frame.flatten()) for brain_frame in brain_seq])\n",
    "  return flat_seq.astype(float)\n",
    "\n",
    "\n",
    "## Loads data from a folder of TIF files\n",
    "# filepath: path to folder\n",
    "# processor: function to process each image\n",
    "# max: max images to load as a proportion of array size\n",
    "# min: min images to load as a proportion of array size\n",
    "# returns: list of processed images, list of filenames\n",
    "def import_data(filepath, processor, min = 0, max = 1):\n",
    "    output_data = []\n",
    "    output_name = []\n",
    "    path_list = os.listdir(filepath)\n",
    "    path_list.sort()\n",
    "    random.Random(4).shuffle(path_list)\n",
    "    min_index = int(min * len(path_list))\n",
    "    max_index = int(max * len(path_list))\n",
    "    for file in itertools.islice(path_list, min_index, max_index):\n",
    "     filename = os.fsdecode(file)\n",
    "     if filename.endswith(\".tif\"):\n",
    "         out = cv2.imreadmulti(filepath + '/' + filename)[1]\n",
    "         output_data.append(processor(out))\n",
    "         output_name.append(filename.split('.')[0])\n",
    "     elif filename.endswith(\".npy\"):\n",
    "         output_data.append(processor(np.load(filepath + '/' + filename)))\n",
    "         output_name.append(filename.split('.')[0])\n",
    "     else:\n",
    "         continue\n",
    "    return output_data, output_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_array(in_array):\n",
    "    return np.array([x / np.linalg.norm(x) for x in in_array])\n",
    "\n",
    "def flatten_data(data):\n",
    "    return np.concatenate(data, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dataloader(brain_data, feature_data, num_steps, time_offset, conditional, batch_size=1, cebra_offset=None ):\n",
    "    datasets = []\n",
    "    print('loading data')\n",
    "    for session in zip(brain_data, feature_data):\n",
    "        brain_data_tensor  = torch.FloatTensor(session[0])\n",
    "        feature_data_tensor = torch.FloatTensor(session[1])\n",
    "        datasets.append(cebra.data.datasets.TensorDataset(brain_data_tensor, continuous=feature_data_tensor, offset=cebra_offset))\n",
    "    dataset_collection = cebra.data.datasets.DatasetCollection(*datasets)\n",
    "    return cebra.data.multi_session.ContinuousMultiSessionDataLoader(\n",
    "        dataset=dataset_collection,\n",
    "        batch_size=batch_size,\n",
    "        num_steps=num_steps,\n",
    "        time_offset=time_offset,\n",
    "        conditional=conditional,\n",
    "    )   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partial_data(brain_paths, behvaior_paths, min, max, num_steps, time_offset, conditional, batch_size, cebra_offset):\n",
    "    brain_data, feature_data = [], []\n",
    "    for path in zip(brain_paths, behvaior_paths):\n",
    "        print('importing from: ' + path[0])\n",
    "        brain_data_temp, _ = import_data(path[0], process_brain, min, max)\n",
    "        feature_data_temp, _ = import_data(path[1], lambda x: np.squeeze(x), min, max)\n",
    "        brain_data.extend((brain_data_temp))\n",
    "        feature_data.extend((feature_data_temp))\n",
    "        del brain_data_temp\n",
    "        del feature_data_temp\n",
    "        del _\n",
    "        gc.collect()\n",
    "    return init_dataloader(brain_data, feature_data, num_steps, time_offset, conditional, batch_size, cebra_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creat and train the model in partial batches of data\n",
    "def partial_train(brain_paths, behavior_paths, min, max, slice_size, num_steps, time_offset, conditional, batch_size, cebra_offset, hidden_units, output_dimension, model_name, device, saved_model = None):\n",
    "    ## Load dataloader for first slice of data\n",
    "    print('Loading data')\n",
    "    dataloader= load_partial_data(brain_paths, behavior_paths, min, min+slice_size, num_steps, time_offset, conditional, batch_size, cebra_offset)\n",
    "    print('Creating model')\n",
    "    ## create list of models\n",
    "    model = torch.nn.ModuleList([\n",
    "    cebra.models.init(model_name, dataset.input_dimension,\n",
    "                        hidden_units, output_dimension, True)\n",
    "    for dataset in dataloader.dataset.iter_sessions()\n",
    "    ]).to(device)\n",
    "    if saved_model is not None:\n",
    "        model.__setstate__(saved_model)\n",
    "\n",
    "    ## Load optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    ## Load criterion\n",
    "    criterion = cebra.models.criterions.LearnableCosineInfoNCE(temperature=1.0, min_temperature=0.01)\n",
    "\n",
    "    print('Loading solver')\n",
    "    ## Load solver and train on first slice of data\n",
    "    solver = cebra.solver.MultiSessionSolver(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        tqdm_on=True,\n",
    "    )\n",
    "    print('Training on slice 1')\n",
    "    solver.fit(dataloader,\n",
    "                save_frequency=500,\n",
    "                logdir='runs',)\n",
    "    solver.save('models/'+model_name+'_slice1.pt')\n",
    "    for i in range(2, int((max-min)/slice_size)+1):\n",
    "        ## Load next slice of data\n",
    "        dataloader= load_partial_data(brain_paths, behavior_paths, min+slice_size*(i-1), min+slice_size*i, num_steps, time_offset, conditional, batch_size, cebra_offset)\n",
    "        ## Train on next slice of data\n",
    "        print('Training on slice '+str(i))\n",
    "        solver.fit(dataloader,\n",
    "                save_frequency=5000,\n",
    "                logdir='runs',)        \n",
    "        torch.save(model, output_model_path)\n",
    "    print('Training complete, saving model')\n",
    "    solver.save(output_model_path)\n",
    "    return solver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2021_1_8_MV1_run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "Creating model\n",
      "Loading solver\n",
      "Training on slice 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.2499 neg:  10.6273 total:  10.8772 temperature:  1.0000:   6%|â–Œ         | 146/2500 [23:01<6:10:47,  9.45s/it]"
     ]
    }
   ],
   "source": [
    "model = partial_train([neural_data_paths[0]],\n",
    "            [dino_paths[0]],\n",
    "            min=0.4,\n",
    "            max=0.8,\n",
    "            slice_size=0.4,\n",
    "            num_steps=2500,\n",
    "            time_offset=10,\n",
    "            conditional='time_delta',\n",
    "            batch_size=128,\n",
    "            cebra_offset=cebra.data.datatypes.Offset(0,1),\n",
    "            hidden_units=128,\n",
    "            output_dimension=8,\n",
    "            model_name='offset1-model',\n",
    "            device='cuda:0'  \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('runs/checkpoint_0009990.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2021_1_8_MV1_run\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2020_11_23_MV1_run\n",
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2020_12_4_MV1_run\n",
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2020_11_2_MV1_run\n",
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2021_1_12_MV1_run\n",
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2020_12_10_MV1_run\n",
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2020_11_9_MV1_run\n",
      "importing from: /mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/brain/2020_11_17_MV1_run\n",
      "loading data\n",
      "Creating model\n",
      "Loading solver\n",
      "Training on slice 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.2769 neg:  9.5146 total:  9.7915 temperature:  1.0000: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [10:48:38<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete, saving model\n"
     ]
    }
   ],
   "source": [
    "# model = partial_train(neural_data_paths,\n",
    "#             dino_paths,\n",
    "#             min=0.4,\n",
    "#             max=0.8,\n",
    "#             valid_max=0.2,\n",
    "#             slice_size=0.4,\n",
    "#             num_steps=10000,\n",
    "#             time_offset=15,\n",
    "#             conditional='time_delta',\n",
    "#             batch_size=4096,\n",
    "#             cebra_offset=cebra.data.datatypes.Offset(0,1),\n",
    "#             hidden_units=128,\n",
    "#             output_dimension=16,\n",
    "#             model_name='offset1-model',\n",
    "#             device='cuda:0',\n",
    "#             saved_model = checkpoint['model']\n",
    "#           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebra-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
