{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, OPTICS\n",
    "from cebra_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cebra_model_path = 'models/single_session/cebra_model_list.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_model_id = { \n",
    "    '2020_11_2_MV1_run' : 0,\n",
    "    '2020_11_9_MV1_run' : 1,\n",
    "    '2021_1_12_MV1_run' : 2,\n",
    "    '2020_12_10_MV1_run' : 3,\n",
    "    '2021_1_8_MV1_run' : 4,\n",
    "    '2020_11_17_MV1_run' : 5,\n",
    "    '2020_11_23_MV1_run' : 6,\n",
    "    '2020_12_4_MV1_run' : 7,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir /mnt/teams/Tsuchitori/IV2_reaching_30hz_30frame_DFF/output5\n",
    "#!ls /mnt/teams/Tsuchitori/IV2_reaching_30hz_30frame_DFF/brain/ | xargs -I % mkdir output5/'%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = '/mnt/teams/Tsuchitori/MV1_run_30hz_30frame_brain2behav_DFF_new/'\n",
    "neural_data_paths = [ data_directory + 'brain/' + \\\n",
    "                     file for file in os.listdir(data_directory + 'brain/')]\n",
    "\n",
    "behavior_data_paths = [  data_directory + 'camera1/' + \\\n",
    "                     file for file in os.listdir(data_directory + 'brain/')]\n",
    "\n",
    "dino_paths = [ data_directory + 'dino/' + \\\n",
    "                        file for file in os.listdir(data_directory + 'brain/')]\n",
    "\n",
    "output_folder_paths = [ data_directory + 'output/' + \\\n",
    "                        file for file in os.listdir(data_directory + 'brain/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_frame_to_embeddings(predicted_embedding, embedding_train, image_train):\n",
    "  cos_dist = np.matmul(embedding_train, predicted_embedding.T)\n",
    "  index_list = np.argmax(cos_dist, axis=0)\n",
    "  return image_train[index_list]\n",
    "\n",
    "def predict_embeddings(neural_path, behavior_path,dino_path, validation_cutoff, valid_size, model, label_model, session):\n",
    "    # Load data\n",
    "    print('Loading data')\n",
    "    brain_data, name_data = import_data(neural_path, lambda x: process_brain(x), max=validation_cutoff)\n",
    "    behavior_data, _ = import_data(behavior_path, lambda x: x, max=validation_cutoff)\n",
    "    dino_data, _ = import_data(dino_path, lambda x: x, max=validation_cutoff)\n",
    "    # Generate embeddings\n",
    "    print('Generating embeddings')\n",
    "    embeddings = [model[session](torch.from_numpy(np.array(x)).float().to('cuda')).to('cpu').detach().numpy() for i, x in enumerate(brain_data)]\n",
    "\n",
    "\n",
    "    print('Loading test data')\n",
    "    # Load the test set of data\n",
    "    brain_data_test, name_data_test = import_data(neural_path, lambda x: process_brain(x), min = validation_cutoff, max=validation_cutoff + valid_size)\n",
    "    behavior_data_test, _ = import_data(behavior_path, lambda x: x, min = validation_cutoff, max=validation_cutoff + valid_size)\n",
    "    dino_data_test, _ = import_data(dino_path, lambda x: x, min = validation_cutoff, max= validation_cutoff + valid_size)\n",
    "    # Generate embeddings\n",
    "    print('Generating test embeddings')\n",
    "    embeddings_test = [model[session](torch.from_numpy(np.array(x)).float().to('cuda')).to('cpu').detach().numpy() for i, x in enumerate(brain_data_test)]\n",
    "\n",
    "    # Flatten Data\n",
    "    embeddings_flat = flatten_data(embeddings).squeeze()\n",
    "    behavior_flat = flatten_data(behavior_data).squeeze()\n",
    "    dino_flat = flatten_data(dino_data).squeeze()\n",
    "    embedding_test_flat = flatten_data(embeddings_test).squeeze()\n",
    "    dino_test_flat = flatten_data(dino_data_test).squeeze()\n",
    "\n",
    "    print('Running KNN')\n",
    "    # Create KNN decoder\n",
    "    decoder = cebra.KNNDecoder(n_neighbors=20, metric=\"cosine\")\n",
    "    decoder.fit(embeddings_flat, dino_flat)\n",
    "\n",
    "    # predict\n",
    "    predicted_dino = decoder.predict(embedding_test_flat)\n",
    "\n",
    "    # normalize predicted embeddings\n",
    "    predicted_dino = normalize_array(predicted_dino)\n",
    "\n",
    "    print('generating labels')\n",
    "    labels = []\n",
    "    for label, data in zip(name_data, brain_data):\n",
    "        if label.split('_')[0] == 'move':\n",
    "            labels.extend(np.tile(np.array([0, 1]),[len(data), 1]))\n",
    "        else:\n",
    "            labels.extend(np.tile(np.array([1, 0]),[len(data), 1]))\n",
    "    labels = np.array(labels)\n",
    "    decoder = cebra.KNNDecoder(n_neighbors=72, metric='cosine')\n",
    "    decoder.fit(embeddings_flat, labels)\n",
    "    predicted_labels = np.argmax(decoder.predict(embedding_test_flat), axis=1)\n",
    "    predicted_labels = reshape_frames(predicted_labels, embeddings_test)\n",
    "    print('generating videos')\n",
    "    # Match predicted embeddings to images\n",
    "    predicted_images = match_frame_to_embeddings(predicted_dino, dino_flat, behavior_flat)\n",
    "    reshaped_predicted_images = reshape_frames(predicted_images, brain_data_test)\n",
    "\n",
    "    return reshaped_predicted_images, name_data_test, behavior_data_test, predicted_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_video(frames, ground_truth, frame_rate, name, output_folder_path):\n",
    "    # Get the dimensions of the frames\n",
    "    frame_height, frame_width = frames[0].shape\n",
    "    # Classify video based on median label of all frames\n",
    "    # Create a VideoWriter object to write the frames into a video file\n",
    "    video_writer = cv2.VideoWriter(output_folder_path + '/' +  name +'.mp4',\n",
    "                                   cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                                   frame_rate,\n",
    "                                   (frame_width, 2 * frame_height))\n",
    "\n",
    "    # Display frames\n",
    "    for iter, frame in enumerate(zip(frames, ground_truth)):\n",
    "        # Write the current frame to the video file\n",
    "        color_frame = cv2.cvtColor(frame[0].astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "        color_truth = cv2.cvtColor(frame[1].astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "        combined = np.concatenate((color_frame, color_truth))\n",
    "        video_writer.write(combined)\n",
    "\n",
    "    # Release the VideoWriter and close the window\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_generated_video(vid, name, output_folder_path):\n",
    "    video_writer = cv2.VideoWriter(output_folder_path + '/' + name +'.mp4',\n",
    "                                   cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                                   24,\n",
    "                                   (64, 64))\n",
    "    # downsample video to 64x64\n",
    "    vid_low = [cv2.resize(frame, (64, 64)) for frame in vid]\n",
    "    for frame in vid_low:\n",
    "        color_frame = cv2.cvtColor(frame.astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "        video_writer.write(color_frame)\n",
    "    video_writer.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_behavior = {\n",
    "    0: 'still',\n",
    "    1: 'sniffing',\n",
    "    2: 'walking',\n",
    "    3: 'grooming',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mmodels/single_session/cebra_model_list.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39meval()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m label_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmodels/cebra_classifier_complete.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(neural_data_paths, behavior_data_paths, dino_paths)):\n",
      "File \u001b[0;32m~/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torch/serialization.py:1116\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1112\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1116\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1121\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torch/serialization.py:187\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mUntypedStorage(obj\u001b[39m.\u001b[39mnbytes(), device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mcuda(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cebra-env/lib/python3.11/site-packages/torch/_utils.py:81\u001b[0m, in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[39mreturn\u001b[39;00m new_type(indices, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m     80\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     untyped_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mUntypedStorage(\n\u001b[1;32m     82\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m     untyped_storage\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('models/single_session/cebra_model_list.pt').eval().to('cuda')\n",
    "label_model = torch.load('models/cebra_classifier_complete.pt')\n",
    "for i, _ in enumerate(zip(neural_data_paths, behavior_data_paths, dino_paths)):\n",
    "    pred_images, names, predict_behavior, pred_labels = predict_embeddings(neural_data_paths[i], behavior_data_paths[i], dino_paths[i], 0.8, 0.2, model, label_model[0], i)\n",
    "    for vid, name, ground_truth, label in zip(pred_images, names, predict_behavior, pred_labels):\n",
    "        windowed_frames = choose_first_second(30, vid)\n",
    "        windowed_truth = choose_first_second(30, ground_truth)\n",
    "        windowed_labels = choose_first_second(30, label)\n",
    "        label = np.round(np.median(windowed_labels))\n",
    "        if label == 0:\n",
    "            name = name + '_pred_no_move'\n",
    "        else:\n",
    "            name = name + '_pred_move'\n",
    "        display_frames_as_video(windowed_frames, windowed_truth, 24, name, output_folder_paths[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebra-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
