{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cebra\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import itertools\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = 'models/cebra_model_complete.pt'\n",
    "data_directory = '/mnt/teams/Tsuchitori/Allen-movie/'\n",
    "neural_data_paths = [ data_directory + 'test_set/']\n",
    "\n",
    "behavior_data_paths = [  data_directory + 'movie/']\n",
    "\n",
    "dino_paths = [ data_directory + 'dino/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_brain(brain_seq):\n",
    "  brain_seq = np.array(brain_seq)\n",
    "  flat_seq = np.array([(brain_frame.flatten()) for brain_frame in brain_seq])\n",
    "  return flat_seq.astype(float)\n",
    "\n",
    "\n",
    "## Loads data from a folder of TIF files\n",
    "# filepath: path to folder\n",
    "# processor: function to process each image\n",
    "# max: max images to load as a proportion of array size\n",
    "# min: min images to load as a proportion of array size\n",
    "# returns: list of processed images, list of filenames\n",
    "def import_data(filepath, processor, min = 0, max = 1):\n",
    "    output_data = []\n",
    "    output_name = []\n",
    "    path_list = os.listdir(filepath)\n",
    "    path_list.sort()\n",
    "    random.Random(4).shuffle(path_list)\n",
    "    min_index = int(min * len(path_list))\n",
    "    max_index = int(max * len(path_list))\n",
    "    for file in itertools.islice(path_list, min_index, max_index):\n",
    "     filename = os.fsdecode(file)\n",
    "     if filename.endswith(\".tif\"):\n",
    "         out = cv2.imreadmulti(filepath + '/' + filename)[1]\n",
    "         output_data.append(processor(out))\n",
    "         output_name.append(filename.split('.')[0])\n",
    "     elif filename.endswith(\".npy\"):\n",
    "         output_data.append(processor(np.load(filepath + '/' + filename)))\n",
    "         output_name.append(filename.split('.')[0])\n",
    "     else:\n",
    "         continue\n",
    "    return output_data, output_name\n",
    "\n",
    "def normalize_array(in_array):\n",
    "    return np.array([x / np.linalg.norm(x) for x in in_array])\n",
    "\n",
    "def flatten_data(data):\n",
    "    return np.concatenate(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits8 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Getting DINO embeddings from behavior data\n",
    "\n",
    "# from https://huggingface.co/facebook/dino-vits8\n",
    "# testing transformer\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained('facebook/dino-vits8')\n",
    "vit_model = ViTModel.from_pretrained('facebook/dino-vits8')\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "vit_model = vit_model.to(device)\n",
    "\n",
    "\n",
    "## Given a DINO model and an image, return the DINO embedding\n",
    "# model: the DINO model\n",
    "# image: a PIL image\n",
    "def get_features(model, image):\n",
    "  return model(**processor(images=image, return_tensors=\"pt\").to(device)).pooler_output.cpu().detach().numpy()\n",
    "\n",
    "## Convert a numpy array to a PIL image\n",
    "# numpy_image: a numpy array\n",
    "# returns: a PIL image\n",
    "def np_to_PIL(numpy_image):\n",
    "    return Image.fromarray(np.uint8(numpy_image)).convert('RGB')\n",
    "\n",
    "## Given a sequence of behavior frames, return a sequence of DINO embeddings\n",
    "# behavior_video: a sequence of behavior frames\n",
    "# model: the DINO model\n",
    "# returns: a sequence of DINO embeddings\n",
    "def get_dino_embeddings(behavior_video, model):\n",
    "  behavior_video = np.array(behavior_video)\n",
    "  feature_sequence = []\n",
    "  for frame in behavior_video:\n",
    "    feature_sequence.append(get_features(model, np_to_PIL(frame)))\n",
    "  return np.array(feature_sequence)\n",
    "\n",
    "# Get DINO embeddings for a set behavior data\n",
    "def get_dino_embeddings_array(behavior_data, model):\n",
    "  dino_embeddings = []\n",
    "  for behavior_video in behavior_data:\n",
    "    dino_embeddings.append(np.squeeze(get_dino_embeddings(behavior_video, model)))\n",
    "  return dino_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dino_embeddings(behavior_data, name_data, model, output_path):\n",
    "  for i, video in enumerate(behavior_data):\n",
    "    # check if embeddings exist first\n",
    "    if output_path + '/' + name_data[i] + '.npy' in os.listdir(output_path):\n",
    "      print('embeddings already found')\n",
    "      pass\n",
    "    else:\n",
    "      dino_embeddings = normalize_array(get_dino_embeddings(video, model))\n",
    "      print(output_path + '/' + name_data[i] + '.npy')\n",
    "      np.save(output_path + '/' + name_data[i] + '.npy', dino_embeddings)\n",
    "  return dino_embeddings\n",
    "\n",
    "def process_behavior_data(behavior_paths, output_paths, model):\n",
    "  for paths in zip(behavior_paths, output_paths):\n",
    "    behavior_data_temp, name_data_temp = import_data(paths[0], lambda x : x, max=1)\n",
    "    print('Saving DINO embeddings for ' + paths[0])\n",
    "    save_dino_embeddings(behavior_data_temp, name_data_temp, model, paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving DINO embeddings for /mnt/teams/Tsuchitori/Allen-movie/movie/\n",
      "/mnt/teams/Tsuchitori/Allen-movie/dino//natural_movie_one.npy\n"
     ]
    }
   ],
   "source": [
    "process_behavior_data(behavior_data_paths, dino_paths, vit_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in neural_data_paths:\n",
    "    _, names = import_data(path, lambda x : x, 0.0, 0.8)\n",
    "    labels = np.array([   'move' == name.split('_')[0] for name in names]).astype(int)\n",
    "    # save each name and path to a txt file\n",
    "    with open('training_names/' + path.split('/')[-1] + '.txt', 'w') as f:\n",
    "        for name, label in zip(names, labels):\n",
    "            f.write(name + ' ' + str(label) + '\\n')\n",
    "\n",
    "for path in neural_data_paths:\n",
    "    _, names = import_data(path, lambda x : x, 0.8, 1)\n",
    "    labels = np.array([   'move' == name.split('_')[0] for name in names]).astype(int)\n",
    "    # save each name and path to a txt file\n",
    "    with open('validation_names/' + path.split('/')[-1] + '.txt', 'w') as f:\n",
    "        for name, label in zip(names, labels):\n",
    "            f.write(name + ' ' + str(label) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cebra-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
